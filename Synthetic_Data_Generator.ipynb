{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOcS2pycoCVoV8od/eIJ+t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aimanalishezan/Synthetic-Data-Generator-Ai/blob/main/Synthetic_Data_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a Model to Generate Synthetic Data"
      ],
      "metadata": {
        "id": "rvBDIZ8Ot-bs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "DwBzS6_lt2bZ"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, BitsAndBytesConfig\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "dTsKWBdGuWCV"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S--kEwWRu7Gm",
        "outputId": "b8eb07bd-b011-4cd6-8f4a-b2add94e7c80"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token= userdata.get('HF_TOKEN')\n",
        "\n",
        "!git config --global credential.helper store\n",
        "\n",
        "login(hf_token,add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "lF15GyjRvC1D"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=userdata.get('model')"
      ],
      "metadata": {
        "id": "MLeAzb9YvTKq"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt=\"You are an expert in generating synthetic datasets. Your goal is to generate realistic datasets \\ based on a given business and its requirements from the user . you will also be given the desired dataset format.\"\n",
        "system_prompt+=\"Do not repeat the instructions.\"\n",
        "\n",
        "user_prompt=(\"Please provide me a dataset for the following business.\"\n",
        "\"For example:\\n\"\n",
        "\"The Business : A retail store selling luxury watches.\\n\"\n",
        "\"The Data Format: CSV.\\n\"\n",
        "\"Output:\\n\"\n",
        "\"Item,Price,Quantity,Brand,Sale Date.\\n\"\n",
        "\"Superocean IT,80,000$, 5, Breitling, 2025-04-10 \\n\"\n",
        "\"If i don't provide you the necessary columns, please create the columns based on your knowledge about the given business\"\n",
        ")"
      ],
      "metadata": {
        "id": "K-nSnrgfwZfW"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_format(data_format, num_records):\n",
        "  format_message=\"\"\n",
        "  if data_format=='CSV':\n",
        "    format_message=\"Please provide the dateset in a CSV format.\"\n",
        "  elif data_format=='JSON':\n",
        "    format_message=\"Please provide the dateset in a JSON format.\"\n",
        "  elif data_format==\"Tabular\":\n",
        "    format_message=\"Please provide the dateset in a tabular format.\"\n",
        "\n",
        "  return format_message + f'please generate {num_records} records '"
      ],
      "metadata": {
        "id": "QpL1peTdwaBF"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def  complete_user_prompt(user_input,data_format,num_records):\n",
        "  messages=[\n",
        "      {\"role\":\"system\",\"content\":system_prompt},\n",
        "      {\"role\":\"user\",\"content\":user_input},\n",
        "      {\"role\":\"user\",\"content\":user_prompt+dataset_format(data_format,num_records)}\n",
        "  ]\n",
        "  return messages"
      ],
      "metadata": {
        "id": "BAZ8mV0TwaMz"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cuda Available\", torch.cuda.is_available())\n",
        "if  torch.cuda.is_available():\n",
        "  print(\"GPU_DEVICE\",torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "else:\n",
        "  print(\"NO Gpu Found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t1lo2B769m7",
        "outputId": "92b39ae2-2886-42ab-8bd0-a57f3beeb1b7"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda Available True\n",
            "GPU_DEVICE Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")"
      ],
      "metadata": {
        "id": "lp3qwoHl8sbb"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_model(model_id, messages):\n",
        "  try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)\n",
        "    inputs=tokenizer.apply_chat_template(messages,return_tensors=\"pt\").to(device=\"cuda\")\n",
        "    streamer = TextStreamer(tokenizer)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=\"auto\")\n",
        "    outpts=model.generate(inputs,max_new_tokens=2000,streamer=streamer)\n",
        "    generated_text=tokenizer.decode(outpts[0],skip_special_tokens=True)\n",
        "    del tokenizer,streamer,model,inputs,outpts\n",
        "    return generated_text\n",
        "  except Exception as e:\n",
        "    return f'Error during generation:{str(e)}'"
      ],
      "metadata": {
        "id": "NyMJEzlg8sm3"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(user_input,target_format,model_choices,num_records):\n",
        "  if model_choices==\"Llama_3.2-8B\":\n",
        "    model_id=model\n",
        "  messages=complete_user_prompt(user_input,target_format,num_records)\n",
        "  generated_text=generate_model(model_id,messages)\n",
        "  return generated_text"
      ],
      "metadata": {
        "id": "J4DR9_tb8spz"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(title=\"Synthetic Data Generator\") as ui:\n",
        "  gr.Markdown(\"## Synthetic Data Generator\")\n",
        "  with gr.Row():\n",
        "    with gr.Column(min_width=600):\n",
        "      user_inputs=gr.Textbox(label=\"Enter your business details data requiremnets\",placeholder=\"Enter here\",lines=15)\n",
        "\n",
        "      model_choices=gr.Dropdown(\n",
        "          ['Llama_3.2-8B'],\n",
        "          label=\"Select Model\",\n",
        "          value=\"Llama_3.2-8B\"\n",
        "      )\n",
        "      target_format=gr.Dropdown(\n",
        "          ['CSV','JSON','Tabular'],\n",
        "          label=\"Select Target Format\",\n",
        "          value=\"CSV\"\n",
        "      )\n",
        "      num_records=gr.Dropdown(\n",
        "          [50,100,200,500,1000],\n",
        "          label=\"Select Number of Records\",\n",
        "          value=50\n",
        "      )\n",
        "      generate_button=gr.Button('Generate')\n",
        "    with gr.Column():\n",
        "      output=gr.Textbox(label='Generate Synthetic Data',lines=30)\n",
        "  generate_button.click(fn=generate_dataset,inputs=[user_inputs,target_format,model_choices,num_records],outputs=output)\n",
        "ui.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "hbJJK2cr_zqi",
        "outputId": "9a2b8c95-c671-47df-b0cf-1d803d22717e"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8450c4f9938c229d26.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8450c4f9938c229d26.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    }
  ]
}